'use server';

if (process.env.NODE_ENV !== "production") {
  console.log('DEBUG ENV KEY:', process.env.OPENAI_API_KEY);
}

import OpenAI from "openai";
import { getPromptForTopic } from "./promptTemplates";

/**
 * åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
 */
export const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

/**
 * å¾åŸå§‹ GPT å›å‚³ä¸­è§£æå‡º JSON é™£åˆ—
 */
function extractJSONArray(raw: string): any[] {
  try {
    const parsed = JSON.parse(raw);
    if (Array.isArray(parsed)) return parsed;
    if (parsed.questions && Array.isArray(parsed.questions)) return parsed.questions;
    return [];
  } catch {
    if (process.env.NODE_ENV !== "production") {
      console.error('âŒ JSON parse error, raw content:', raw);
    }
    return [];
  }
}

/**
 * èˆŠç‰ˆï¼šç›´æ¥å¾ç´”æ–‡å­— context ç”Ÿæˆé¡Œç›®é™£åˆ—
 */
export async function generateQuestionsFromText(
  content: string,
  settings?: {
    difficulty?: string;
    questionType?: string;
    tone?: string;
    length?: string;
  },
  model: string = "gpt-4"
): Promise<any[]> {
  const prompt = getPromptForTopic(content, settings);

  const response = await openai.chat.completions.create({
    model,
    messages: [
      { role: "system", content: "ä½ æ˜¯ä¸€ä½åªèƒ½è¼¸å‡ºç´” JSON æ ¼å¼é¡Œç›®çš„ AI åŠ©æ•™ã€‚" },
      { role: "user", content: prompt },
    ],
    temperature: 0.7,
  });

  const raw = response.choices?.[0]?.message?.content || "";

  if (process.env.NODE_ENV !== "production") {
    console.log("ğŸ§  GPT å›å‚³åŸå§‹å…§å®¹ â†“â†“â†“");
    console.log(raw);
  }

  const parsed = extractJSONArray(raw);
  if (parsed.length === 0) {
    console.warn("âš ï¸ GPT å›å‚³æ ¼å¼éŒ¯èª¤ï¼Œç„¡æ³•è§£æç‚ºé¡Œç›®");
  }

  return parsed;
}

/**
 * æ–°ç‰ˆï¼šé€šç”¨å°è£ï¼Œæ”¯æ´å¤šæ¨¡å‹é¸æ“‡
 */
export async function callGPT(opts: {
  prompt: string;
  temperature?: number;
  model?: string; // æ”¯æ´å¤šç¨®æ¨¡å‹ï¼Œå¦‚ gpt-4.1, gpt-3.5-turbo-16k, o4-mini ç­‰
}): Promise<string> {
  const { prompt, temperature = 0.7, model = "gpt-3.5-turbo" } = opts;
  const res = await openai.chat.completions.create({
    model,
    temperature,
    messages: [
      { role: "system", content: "ä½ æ˜¯ä¸€å€‹æœ‰æ¢ç†ä¸”ç²¾æº–çš„ AI åŠ©æ‰‹ã€‚" },
      { role: "user", content: prompt },
    ],
  });
  return res.choices?.[0]?.message?.content?.trim() ?? "";
}
